{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b77e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import gymnasium as gym\n",
    "from pathlib import Path\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from rl_lander_dpg_td3 import (\n",
    "    train_model,\n",
    "    rollout_and_bias,\n",
    "    ENV_ID,\n",
    "    EVAL_EPISODES,\n",
    ")\n",
    "\n",
    "ALGOS = [\"DDPG\", \"TD3\"]\n",
    "SEEDS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # 10 seeds\n",
    "TIMESTEPS = 10_000  # use less for speed while iterating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07b9e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DDPG | seed=0 ===\n",
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bw/Desktop/Dev/SU/IAR/mini-projet-iar/.venv/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to runs/study_1760693288/DDPG_seed0/DDPG_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -472     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 261      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 414      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.89     |\n",
      "|    critic_loss     | 44.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 313      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -348     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 236      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 914      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.71     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 813      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -334     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 224      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1446     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 16       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-265.05 +/- 88.95\n",
      "Episode length: 189.60 +/- 74.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 8.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | -295     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 214      |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 2090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 209      |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 2832     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2731     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -251     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 206      |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 3753     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.66     |\n",
      "|    critic_loss     | 7.93     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3652     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-112.19 +/- 63.19\n",
      "Episode length: 227.80 +/- 37.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 14       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 155      |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 205      |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 4352     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4251     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 174      |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 204      |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 5579     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.28     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5478     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-55.21 +/- 158.49\n",
      "Episode length: 382.60 +/- 333.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 383      |\n",
      "|    mean_reward     | -55.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.74     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 202      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 7054     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.4    |\n",
      "|    critic_loss     | 5        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6953     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-66.71 +/- 57.26\n",
      "Episode length: 284.40 +/- 166.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 284      |\n",
      "|    mean_reward     | -66.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 8063     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 39.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7962     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 8855     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 4.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8754     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-45.39 +/- 40.46\n",
      "Episode length: 308.00 +/- 348.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 308      |\n",
      "|    mean_reward     | -45.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.9    |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bw/Desktop/Dev/SU/IAR/mini-projet-iar/.venv/lib/python3.13/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DDPG | seed=1 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/DDPG_seed1/DDPG_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -472     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 271      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 414      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.89     |\n",
      "|    critic_loss     | 44.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 313      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -348     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 230      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 914      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.71     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 813      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -334     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 221      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1446     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 16       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-265.05 +/- 88.95\n",
      "Episode length: 189.60 +/- 74.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 8.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | -295     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 215      |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 2090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 214      |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 2832     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2731     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -251     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 214      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 3753     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.66     |\n",
      "|    critic_loss     | 7.93     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3652     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-112.19 +/- 63.19\n",
      "Episode length: 227.80 +/- 37.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 14       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 155      |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 212      |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 4352     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4251     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 174      |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 213      |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 5579     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.28     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5478     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-55.21 +/- 158.49\n",
      "Episode length: 382.60 +/- 333.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 383      |\n",
      "|    mean_reward     | -55.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.74     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 211      |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 7054     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.4    |\n",
      "|    critic_loss     | 5        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6953     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-66.71 +/- 57.26\n",
      "Episode length: 284.40 +/- 166.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 284      |\n",
      "|    mean_reward     | -66.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 211      |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 8063     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 39.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7962     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 210      |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 8855     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 4.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8754     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-45.39 +/- 40.46\n",
      "Episode length: 308.00 +/- 348.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 308      |\n",
      "|    mean_reward     | -45.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.9    |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "\n",
      "=== DDPG | seed=2 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/DDPG_seed2/DDPG_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -472     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 254      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 414      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.89     |\n",
      "|    critic_loss     | 44.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 313      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -348     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 221      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 914      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.71     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 813      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -334     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 217      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1446     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 16       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-265.05 +/- 88.95\n",
      "Episode length: 189.60 +/- 74.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 8.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | -295     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 202      |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 2090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 203      |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 2832     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2731     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -251     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 201      |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 3753     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.66     |\n",
      "|    critic_loss     | 7.93     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3652     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-112.19 +/- 63.19\n",
      "Episode length: 227.80 +/- 37.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 14       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 155      |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 200      |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 4352     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4251     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 174      |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 203      |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 5579     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.28     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5478     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-55.21 +/- 158.49\n",
      "Episode length: 382.60 +/- 333.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 383      |\n",
      "|    mean_reward     | -55.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.74     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 205      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 7054     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.4    |\n",
      "|    critic_loss     | 5        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6953     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-66.71 +/- 57.26\n",
      "Episode length: 284.40 +/- 166.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 284      |\n",
      "|    mean_reward     | -66.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 206      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 8063     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 39.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7962     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 208      |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 8855     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 4.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8754     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-45.39 +/- 40.46\n",
      "Episode length: 308.00 +/- 348.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 308      |\n",
      "|    mean_reward     | -45.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.9    |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "\n",
      "=== DDPG | seed=3 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/DDPG_seed3/DDPG_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -472     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 289      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 414      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.89     |\n",
      "|    critic_loss     | 44.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 313      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -348     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 244      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 914      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.71     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 813      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -334     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 231      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1446     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 16       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-265.05 +/- 88.95\n",
      "Episode length: 189.60 +/- 74.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 8.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | -295     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 2090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 2832     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2731     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -251     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 3753     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.66     |\n",
      "|    critic_loss     | 7.93     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3652     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-112.19 +/- 63.19\n",
      "Episode length: 227.80 +/- 37.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 14       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 155      |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 187      |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 4352     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4251     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 174      |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 189      |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 5579     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.28     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5478     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-55.21 +/- 158.49\n",
      "Episode length: 382.60 +/- 333.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 383      |\n",
      "|    mean_reward     | -55.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.74     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 7054     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.4    |\n",
      "|    critic_loss     | 5        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6953     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-66.71 +/- 57.26\n",
      "Episode length: 284.40 +/- 166.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 284      |\n",
      "|    mean_reward     | -66.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 8063     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 39.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7962     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 8855     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 4.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8754     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-45.39 +/- 40.46\n",
      "Episode length: 308.00 +/- 348.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 308      |\n",
      "|    mean_reward     | -45.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.9    |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "\n",
      "=== DDPG | seed=4 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/DDPG_seed4/DDPG_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -472     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 250      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 414      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.89     |\n",
      "|    critic_loss     | 44.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 313      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -348     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 208      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 914      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.71     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 813      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -334     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 201      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 1446     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 16       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-265.05 +/- 88.95\n",
      "Episode length: 189.60 +/- 74.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 8.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | -295     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 189      |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 2090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 189      |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 2832     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2731     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -251     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 189      |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 3753     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.66     |\n",
      "|    critic_loss     | 7.93     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3652     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-112.19 +/- 63.19\n",
      "Episode length: 227.80 +/- 37.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 14       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 155      |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 187      |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 4352     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4251     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 174      |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 187      |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 5579     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.28     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5478     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-55.21 +/- 158.49\n",
      "Episode length: 382.60 +/- 333.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 383      |\n",
      "|    mean_reward     | -55.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.74     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 7054     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.4    |\n",
      "|    critic_loss     | 5        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6953     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-66.71 +/- 57.26\n",
      "Episode length: 284.40 +/- 166.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 284      |\n",
      "|    mean_reward     | -66.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 8063     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 39.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7962     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 8855     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 4.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8754     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-45.39 +/- 40.46\n",
      "Episode length: 308.00 +/- 348.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 308      |\n",
      "|    mean_reward     | -45.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.9    |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "\n",
      "=== DDPG | seed=5 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/DDPG_seed5/DDPG_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -472     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 283      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 414      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.89     |\n",
      "|    critic_loss     | 44.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 313      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -348     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 210      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 914      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.71     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 813      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -334     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 1446     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 16       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-265.05 +/- 88.95\n",
      "Episode length: 189.60 +/- 74.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 8.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | -295     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 2090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 201      |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 2832     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2731     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -251     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 205      |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 3753     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.66     |\n",
      "|    critic_loss     | 7.93     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3652     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-112.19 +/- 63.19\n",
      "Episode length: 227.80 +/- 37.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 14       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 155      |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 206      |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 4352     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4251     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 174      |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 204      |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 5579     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.28     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5478     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-55.21 +/- 158.49\n",
      "Episode length: 382.60 +/- 333.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 383      |\n",
      "|    mean_reward     | -55.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.74     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 205      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 7054     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.4    |\n",
      "|    critic_loss     | 5        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6953     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-66.71 +/- 57.26\n",
      "Episode length: 284.40 +/- 166.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 284      |\n",
      "|    mean_reward     | -66.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 205      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 8063     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 39.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7962     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 206      |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 8855     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 4.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8754     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-45.39 +/- 40.46\n",
      "Episode length: 308.00 +/- 348.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 308      |\n",
      "|    mean_reward     | -45.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.9    |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "\n",
      "=== DDPG | seed=6 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/DDPG_seed6/DDPG_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -472     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 282      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 414      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.89     |\n",
      "|    critic_loss     | 44.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 313      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -348     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 244      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 914      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.71     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 813      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -334     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 235      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1446     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 16       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-265.05 +/- 88.95\n",
      "Episode length: 189.60 +/- 74.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 8.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | -295     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 228      |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 2090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 228      |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 2832     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2731     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -251     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 226      |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 3753     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.66     |\n",
      "|    critic_loss     | 7.93     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3652     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-112.19 +/- 63.19\n",
      "Episode length: 227.80 +/- 37.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 14       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 155      |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 224      |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 4352     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4251     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 174      |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 222      |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 5579     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.28     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5478     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-55.21 +/- 158.49\n",
      "Episode length: 382.60 +/- 333.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 383      |\n",
      "|    mean_reward     | -55.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.74     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 219      |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 7054     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.4    |\n",
      "|    critic_loss     | 5        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6953     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-66.71 +/- 57.26\n",
      "Episode length: 284.40 +/- 166.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 284      |\n",
      "|    mean_reward     | -66.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 218      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 8063     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 39.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7962     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 219      |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 8855     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 4.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8754     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-45.39 +/- 40.46\n",
      "Episode length: 308.00 +/- 348.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 308      |\n",
      "|    mean_reward     | -45.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.9    |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "\n",
      "=== DDPG | seed=7 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/DDPG_seed7/DDPG_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -472     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 292      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 414      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.89     |\n",
      "|    critic_loss     | 44.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 313      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -348     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 243      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 914      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.71     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 813      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -334     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 237      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1446     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 16       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-265.05 +/- 88.95\n",
      "Episode length: 189.60 +/- 74.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 8.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | -295     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 230      |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 2090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 229      |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 2832     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2731     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -251     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 228      |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 3753     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.66     |\n",
      "|    critic_loss     | 7.93     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3652     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-112.19 +/- 63.19\n",
      "Episode length: 227.80 +/- 37.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 14       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 155      |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 226      |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 4352     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4251     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 174      |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 226      |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 5579     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.28     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5478     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-55.21 +/- 158.49\n",
      "Episode length: 382.60 +/- 333.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 383      |\n",
      "|    mean_reward     | -55.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.74     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 224      |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 7054     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.4    |\n",
      "|    critic_loss     | 5        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6953     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-66.71 +/- 57.26\n",
      "Episode length: 284.40 +/- 166.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 284      |\n",
      "|    mean_reward     | -66.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 223      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 8063     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 39.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7962     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 223      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 8855     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 4.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8754     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-45.39 +/- 40.46\n",
      "Episode length: 308.00 +/- 348.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 308      |\n",
      "|    mean_reward     | -45.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.9    |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "\n",
      "=== DDPG | seed=8 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/DDPG_seed8/DDPG_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -472     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 250      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 414      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.89     |\n",
      "|    critic_loss     | 44.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 313      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -348     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 222      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 914      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.71     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 813      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -334     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 206      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1446     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 16       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-265.05 +/- 88.95\n",
      "Episode length: 189.60 +/- 74.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 8.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | -295     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 200      |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 2090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 200      |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 2832     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2731     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -251     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 199      |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 3753     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.66     |\n",
      "|    critic_loss     | 7.93     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3652     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-112.19 +/- 63.19\n",
      "Episode length: 227.80 +/- 37.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 14       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 155      |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 4352     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4251     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 174      |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 5579     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.28     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5478     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-55.21 +/- 158.49\n",
      "Episode length: 382.60 +/- 333.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 383      |\n",
      "|    mean_reward     | -55.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.74     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 7054     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.4    |\n",
      "|    critic_loss     | 5        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6953     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-66.71 +/- 57.26\n",
      "Episode length: 284.40 +/- 166.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 284      |\n",
      "|    mean_reward     | -66.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 8063     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 39.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7962     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 199      |\n",
      "|    time_elapsed    | 44       |\n",
      "|    total_timesteps | 8855     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 4.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8754     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-45.39 +/- 40.46\n",
      "Episode length: 308.00 +/- 348.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 308      |\n",
      "|    mean_reward     | -45.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.9    |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "\n",
      "=== DDPG | seed=9 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/DDPG_seed9/DDPG_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -472     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 249      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 414      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.89     |\n",
      "|    critic_loss     | 44.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 313      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -348     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 213      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 914      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.71     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 813      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -334     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 206      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1446     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 16       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-265.05 +/- 88.95\n",
      "Episode length: 189.60 +/- 74.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | -265     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 12.2     |\n",
      "|    critic_loss     | 8.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | -295     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 200      |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 2090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 199      |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 2832     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2731     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -251     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 199      |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 3753     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.66     |\n",
      "|    critic_loss     | 7.93     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3652     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-112.19 +/- 63.19\n",
      "Episode length: 227.80 +/- 37.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.2     |\n",
      "|    critic_loss     | 14       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 155      |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 4352     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4251     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 174      |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 5579     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.28     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5478     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-55.21 +/- 158.49\n",
      "Episode length: 382.60 +/- 333.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 383      |\n",
      "|    mean_reward     | -55.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.74     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 188      |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 7054     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.4    |\n",
      "|    critic_loss     | 5        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6953     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-66.71 +/- 57.26\n",
      "Episode length: 284.40 +/- 166.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 284      |\n",
      "|    mean_reward     | -66.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 8063     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 39.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7962     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 8855     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 4.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8754     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-45.39 +/- 40.46\n",
      "Episode length: 308.00 +/- 348.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 308      |\n",
      "|    mean_reward     | -45.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.9    |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "\n",
      "=== TD3 | seed=0 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/TD3_seed0/TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -484     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 237      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 401      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.32     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | -498     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 200      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1337     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.83     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1236     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-233.18 +/- 162.49\n",
      "Episode length: 667.20 +/- 317.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 667      |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.96     |\n",
      "|    critic_loss     | 55.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 260      |\n",
      "|    ep_rew_mean     | -410     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 3120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.85     |\n",
      "|    critic_loss     | 15       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-100.58 +/- 89.31\n",
      "Episode length: 498.40 +/- 409.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 498      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.33     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 301      |\n",
      "|    ep_rew_mean     | -327     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 4816     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.63     |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4715     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 293      |\n",
      "|    ep_rew_mean     | -289     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 187      |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 5858     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.22     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5757     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-186.52 +/- 53.15\n",
      "Episode length: 317.60 +/- 42.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 318      |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.1      |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 7284     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.93     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7183     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-214.80 +/- 27.95\n",
      "Episode length: 213.80 +/- 19.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 214      |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.39     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | -273     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 44       |\n",
      "|    total_timesteps | 8320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.09     |\n",
      "|    critic_loss     | 9.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8219     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 187      |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 9178     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9077     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 189      |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 9982     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.6      |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9881     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-109.14 +/- 40.92\n",
      "Episode length: 207.40 +/- 59.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 207      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.24     |\n",
      "|    critic_loss     | 9.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "\n",
      "=== TD3 | seed=1 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/TD3_seed1/TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -484     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 229      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 401      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.32     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | -498     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1337     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.83     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1236     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-233.18 +/- 162.49\n",
      "Episode length: 667.20 +/- 317.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 667      |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.96     |\n",
      "|    critic_loss     | 55.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 260      |\n",
      "|    ep_rew_mean     | -410     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 182      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 3120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.85     |\n",
      "|    critic_loss     | 15       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-100.58 +/- 89.31\n",
      "Episode length: 498.40 +/- 409.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 498      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.33     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 301      |\n",
      "|    ep_rew_mean     | -327     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 4816     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.63     |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4715     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 293      |\n",
      "|    ep_rew_mean     | -289     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 143      |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 5858     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.22     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5757     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-186.52 +/- 53.15\n",
      "Episode length: 317.60 +/- 42.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 318      |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.1      |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 7284     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.93     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7183     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-214.80 +/- 27.95\n",
      "Episode length: 213.80 +/- 19.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 214      |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.39     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | -273     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 8320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.09     |\n",
      "|    critic_loss     | 9.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8219     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 9178     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9077     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 9982     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.6      |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9881     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-109.14 +/- 40.92\n",
      "Episode length: 207.40 +/- 59.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 207      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.24     |\n",
      "|    critic_loss     | 9.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "\n",
      "=== TD3 | seed=2 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/TD3_seed2/TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -484     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 202      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 401      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.32     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | -498     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 176      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 1337     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.83     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1236     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-233.18 +/- 162.49\n",
      "Episode length: 667.20 +/- 317.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 667      |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.96     |\n",
      "|    critic_loss     | 55.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 260      |\n",
      "|    ep_rew_mean     | -410     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 175      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 3120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.85     |\n",
      "|    critic_loss     | 15       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-100.58 +/- 89.31\n",
      "Episode length: 498.40 +/- 409.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 498      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.33     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 301      |\n",
      "|    ep_rew_mean     | -327     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 4816     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.63     |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4715     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 293      |\n",
      "|    ep_rew_mean     | -289     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 5858     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.22     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5757     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-186.52 +/- 53.15\n",
      "Episode length: 317.60 +/- 42.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 318      |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.1      |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 7284     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.93     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7183     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-214.80 +/- 27.95\n",
      "Episode length: 213.80 +/- 19.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 214      |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.39     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | -273     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 175      |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 8320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.09     |\n",
      "|    critic_loss     | 9.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8219     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 175      |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 9178     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9077     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 175      |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 9982     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.6      |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9881     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-109.14 +/- 40.92\n",
      "Episode length: 207.40 +/- 59.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 207      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.24     |\n",
      "|    critic_loss     | 9.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "\n",
      "=== TD3 | seed=3 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/TD3_seed3/TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -484     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 223      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 401      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.32     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | -498     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 1337     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.83     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1236     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-233.18 +/- 162.49\n",
      "Episode length: 667.20 +/- 317.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 667      |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.96     |\n",
      "|    critic_loss     | 55.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 260      |\n",
      "|    ep_rew_mean     | -410     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 3120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.85     |\n",
      "|    critic_loss     | 15       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-100.58 +/- 89.31\n",
      "Episode length: 498.40 +/- 409.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 498      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.33     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 301      |\n",
      "|    ep_rew_mean     | -327     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 4816     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.63     |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4715     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 293      |\n",
      "|    ep_rew_mean     | -289     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 5858     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.22     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5757     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-186.52 +/- 53.15\n",
      "Episode length: 317.60 +/- 42.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 318      |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.1      |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 7284     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.93     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7183     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-214.80 +/- 27.95\n",
      "Episode length: 213.80 +/- 19.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 214      |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.39     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | -273     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 8320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.09     |\n",
      "|    critic_loss     | 9.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8219     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 9178     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9077     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 174      |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 9982     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.6      |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9881     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-109.14 +/- 40.92\n",
      "Episode length: 207.40 +/- 59.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 207      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.24     |\n",
      "|    critic_loss     | 9.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "\n",
      "=== TD3 | seed=4 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/TD3_seed4/TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -484     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 178      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 401      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.32     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | -498     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 1337     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.83     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1236     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-233.18 +/- 162.49\n",
      "Episode length: 667.20 +/- 317.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 667      |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.96     |\n",
      "|    critic_loss     | 55.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 260      |\n",
      "|    ep_rew_mean     | -410     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 181      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 3120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.85     |\n",
      "|    critic_loss     | 15       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-100.58 +/- 89.31\n",
      "Episode length: 498.40 +/- 409.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 498      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.33     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 301      |\n",
      "|    ep_rew_mean     | -327     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 182      |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 4816     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.63     |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4715     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 293      |\n",
      "|    ep_rew_mean     | -289     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 181      |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 5858     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.22     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5757     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-186.52 +/- 53.15\n",
      "Episode length: 317.60 +/- 42.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 318      |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.1      |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 182      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 7284     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.93     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7183     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-214.80 +/- 27.95\n",
      "Episode length: 213.80 +/- 19.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 214      |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.39     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | -273     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 181      |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 8320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.09     |\n",
      "|    critic_loss     | 9.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8219     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 183      |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 9178     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9077     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 183      |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 9982     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.6      |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9881     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-109.14 +/- 40.92\n",
      "Episode length: 207.40 +/- 59.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 207      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.24     |\n",
      "|    critic_loss     | 9.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "\n",
      "=== TD3 | seed=5 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/TD3_seed5/TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -484     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 253      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 401      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.32     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | -498     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 210      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1337     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.83     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1236     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-233.18 +/- 162.49\n",
      "Episode length: 667.20 +/- 317.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 667      |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.96     |\n",
      "|    critic_loss     | 55.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 260      |\n",
      "|    ep_rew_mean     | -410     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 3120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.85     |\n",
      "|    critic_loss     | 15       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-100.58 +/- 89.31\n",
      "Episode length: 498.40 +/- 409.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 498      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.33     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 301      |\n",
      "|    ep_rew_mean     | -327     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 4816     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.63     |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4715     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 293      |\n",
      "|    ep_rew_mean     | -289     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 5858     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.22     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5757     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-186.52 +/- 53.15\n",
      "Episode length: 317.60 +/- 42.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 318      |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.1      |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 7284     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.93     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7183     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-214.80 +/- 27.95\n",
      "Episode length: 213.80 +/- 19.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 214      |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.39     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | -273     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 8320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.09     |\n",
      "|    critic_loss     | 9.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8219     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 9178     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9077     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 9982     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.6      |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9881     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-109.14 +/- 40.92\n",
      "Episode length: 207.40 +/- 59.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 207      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.24     |\n",
      "|    critic_loss     | 9.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "\n",
      "=== TD3 | seed=6 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/TD3_seed6/TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -484     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 264      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 401      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.32     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | -498     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 216      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1337     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.83     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1236     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-233.18 +/- 162.49\n",
      "Episode length: 667.20 +/- 317.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 667      |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.96     |\n",
      "|    critic_loss     | 55.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 260      |\n",
      "|    ep_rew_mean     | -410     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 183      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 3120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.85     |\n",
      "|    critic_loss     | 15       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-100.58 +/- 89.31\n",
      "Episode length: 498.40 +/- 409.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 498      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.33     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 301      |\n",
      "|    ep_rew_mean     | -327     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 184      |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 4816     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.63     |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4715     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 293      |\n",
      "|    ep_rew_mean     | -289     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 187      |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 5858     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.22     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5757     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-186.52 +/- 53.15\n",
      "Episode length: 317.60 +/- 42.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 318      |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.1      |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 189      |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 7284     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.93     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7183     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-214.80 +/- 27.95\n",
      "Episode length: 213.80 +/- 19.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 214      |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.39     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | -273     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 189      |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 8320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.09     |\n",
      "|    critic_loss     | 9.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8219     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 9178     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9077     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 9982     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.6      |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9881     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-109.14 +/- 40.92\n",
      "Episode length: 207.40 +/- 59.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 207      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.24     |\n",
      "|    critic_loss     | 9.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "\n",
      "=== TD3 | seed=7 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/TD3_seed7/TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -484     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 258      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 401      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.32     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | -498     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 206      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1337     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.83     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1236     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-233.18 +/- 162.49\n",
      "Episode length: 667.20 +/- 317.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 667      |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.96     |\n",
      "|    critic_loss     | 55.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 260      |\n",
      "|    ep_rew_mean     | -410     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 3120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.85     |\n",
      "|    critic_loss     | 15       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-100.58 +/- 89.31\n",
      "Episode length: 498.40 +/- 409.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 498      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.33     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 301      |\n",
      "|    ep_rew_mean     | -327     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 184      |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 4816     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.63     |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4715     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 293      |\n",
      "|    ep_rew_mean     | -289     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 184      |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 5858     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.22     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5757     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-186.52 +/- 53.15\n",
      "Episode length: 317.60 +/- 42.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 318      |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.1      |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 181      |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 7284     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.93     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7183     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-214.80 +/- 27.95\n",
      "Episode length: 213.80 +/- 19.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 214      |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.39     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | -273     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 8320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.09     |\n",
      "|    critic_loss     | 9.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8219     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 9178     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9077     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 9982     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.6      |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9881     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-109.14 +/- 40.92\n",
      "Episode length: 207.40 +/- 59.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 207      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.24     |\n",
      "|    critic_loss     | 9.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "\n",
      "=== TD3 | seed=8 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/TD3_seed8/TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -484     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 274      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 401      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.32     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | -498     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 220      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1337     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.83     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1236     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-233.18 +/- 162.49\n",
      "Episode length: 667.20 +/- 317.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 667      |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.96     |\n",
      "|    critic_loss     | 55.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 260      |\n",
      "|    ep_rew_mean     | -410     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 201      |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 3120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.85     |\n",
      "|    critic_loss     | 15       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-100.58 +/- 89.31\n",
      "Episode length: 498.40 +/- 409.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 498      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.33     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 301      |\n",
      "|    ep_rew_mean     | -327     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 4816     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.63     |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4715     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 293      |\n",
      "|    ep_rew_mean     | -289     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 5858     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.22     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5757     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-186.52 +/- 53.15\n",
      "Episode length: 317.60 +/- 42.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 318      |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.1      |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 7284     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.93     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7183     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-214.80 +/- 27.95\n",
      "Episode length: 213.80 +/- 19.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 214      |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.39     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | -273     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 8320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.09     |\n",
      "|    critic_loss     | 9.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8219     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 9178     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9077     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 9982     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.6      |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9881     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-109.14 +/- 40.92\n",
      "Episode length: 207.40 +/- 59.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 207      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.24     |\n",
      "|    critic_loss     | 9.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "\n",
      "=== TD3 | seed=9 ===\n",
      "Using cpu device\n",
      "Logging to runs/study_1760693288/TD3_seed9/TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -484     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 255      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 401      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.32     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | -498     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 210      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1337     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.83     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1236     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-233.18 +/- 162.49\n",
      "Episode length: 667.20 +/- 317.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 667      |\n",
      "|    mean_reward     | -233     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.96     |\n",
      "|    critic_loss     | 55.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 260      |\n",
      "|    ep_rew_mean     | -410     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 201      |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 3120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.85     |\n",
      "|    critic_loss     | 15       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-100.58 +/- 89.31\n",
      "Episode length: 498.40 +/- 409.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 498      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.33     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 301      |\n",
      "|    ep_rew_mean     | -327     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 199      |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 4816     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.63     |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4715     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 293      |\n",
      "|    ep_rew_mean     | -289     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 201      |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 5858     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.22     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5757     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-186.52 +/- 53.15\n",
      "Episode length: 317.60 +/- 42.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 318      |\n",
      "|    mean_reward     | -187     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.1      |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 199      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 7284     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.93     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7183     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-214.80 +/- 27.95\n",
      "Episode length: 213.80 +/- 19.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 214      |\n",
      "|    mean_reward     | -215     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.39     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | -273     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 8320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.09     |\n",
      "|    critic_loss     | 9.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8219     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 9178     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9077     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 9982     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.6      |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9881     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-109.14 +/- 40.92\n",
      "Episode length: 207.40 +/- 59.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 207      |\n",
      "|    mean_reward     | -109     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.24     |\n",
      "|    critic_loss     | 9.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "\n",
      "Saved summary to: runs/study_1760693288/summary.json\n"
     ]
    }
   ],
   "source": [
    "stamp = int(time.time())\n",
    "root = Path(f\"runs/study_{stamp}\")\n",
    "root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "summary = []\n",
    "for alg in ALGOS:\n",
    "    for i, s in enumerate(SEEDS):\n",
    "        print(f\"\\n=== {alg} | seed={s} ===\")\n",
    "        logdir = root / f\"{alg}_seed{s}\"\n",
    "        logdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # train (reusing your function; set global SEED-like behavior)\n",
    "        model, path = train_model(alg, str(logdir), TIMESTEPS)\n",
    "\n",
    "        # eval\n",
    "        mean, std = evaluate_policy(\n",
    "            model,\n",
    "            gym.make(ENV_ID),\n",
    "            n_eval_episodes=EVAL_EPISODES,\n",
    "            deterministic=True,\n",
    "        )\n",
    "        # bias (held-out rollouts)\n",
    "        bias = rollout_and_bias(model, episodes=EVAL_EPISODES)\n",
    "\n",
    "        rec = {\n",
    "            \"algo\": alg,\n",
    "            \"seed\": s,\n",
    "            \"timesteps\": TIMESTEPS,\n",
    "            \"eval_mean\": float(mean),\n",
    "            \"eval_std\": float(std),\n",
    "            \"bias_metrics\": bias,  # contains mean_return, std_return, and bias stats\n",
    "            \"model_path\": path,\n",
    "            \"logdir\": str(logdir),\n",
    "        }\n",
    "        summary.append(rec)\n",
    "\n",
    "        with open(logdir / \"result.json\", \"w\") as f:\n",
    "            json.dump(rec, f, indent=2)\n",
    "\n",
    "with open(root / \"summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved summary to: {root/'summary.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db706cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
