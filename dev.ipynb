{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecfbf116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3 import DDPG, TD3\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    EvalCallback,\n",
    "    StopTrainingOnRewardThreshold,\n",
    ")\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "import torch\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "TIMESTEPS = 60_000  # adjustable\n",
    "EVAL_EPISODES = 20\n",
    "GAMMA = 0.99\n",
    "ENV_ID = \"LunarLanderContinuous-v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ba443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(seed=SEED):\n",
    "    def _thunk():\n",
    "        env = gym.make(ENV_ID)\n",
    "        env = TimeLimit(env, max_episode_steps=1000)\n",
    "        env.reset(seed=seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "\n",
    "    return _thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(alg: str, logdir: str, timesteps: int) -> Tuple[object, str]:\n",
    "    env = DummyVecEnv([make_env(SEED)])\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "\n",
    "    # exploration noise for deterministic policies\n",
    "    action_noise = NormalActionNoise(\n",
    "        mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions)\n",
    "    )\n",
    "\n",
    "    if alg == \"DDPG\":\n",
    "        model = DDPG(\n",
    "            \"MlpPolicy\",\n",
    "            env,\n",
    "            learning_rate=3e-4,\n",
    "            buffer_size=1_000_000,\n",
    "            batch_size=256,\n",
    "            gamma=GAMMA,\n",
    "            tau=0.005,\n",
    "            train_freq=(1, \"step\"),\n",
    "            gradient_steps=1,\n",
    "            action_noise=action_noise,\n",
    "            tensorboard_log=logdir,\n",
    "            seed=SEED,\n",
    "            verbose=1,\n",
    "        )\n",
    "    elif alg == \"TD3\":\n",
    "        model = TD3(\n",
    "            \"MlpPolicy\",\n",
    "            env,\n",
    "            learning_rate=3e-4,\n",
    "            buffer_size=1_000_000,\n",
    "            batch_size=256,\n",
    "            gamma=GAMMA,\n",
    "            tau=0.005,\n",
    "            train_freq=(1, \"step\"),\n",
    "            gradient_steps=1,\n",
    "            action_noise=action_noise,\n",
    "            tensorboard_log=logdir,\n",
    "            seed=SEED,\n",
    "            verbose=1,\n",
    "            policy_delay=2,\n",
    "            target_policy_noise=0.2,\n",
    "            target_noise_clip=0.5,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"alg must be 'DDPG' or 'TD3'\")\n",
    "\n",
    "    # evaluate while training\n",
    "    eval_env = make_env(SEED + 1)()\n",
    "    stop_cb = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
    "    eval_cb = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=os.path.join(logdir, f\"{alg}_best\"),\n",
    "        log_path=os.path.join(logdir, f\"{alg}_eval\"),\n",
    "        eval_freq=10_000,\n",
    "        n_eval_episodes=5,\n",
    "        deterministic=True,\n",
    "        callback_on_new_best=stop_cb,\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=timesteps, callback=eval_cb)\n",
    "\n",
    "    model.save(os.path.join(logdir, f\"{alg}_final\"))\n",
    "    env.close()\n",
    "    eval_env.close()\n",
    "    return model, os.path.join(logdir, f\"{alg}_final.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce6287",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def critic_q_estimate(model, obs: np.ndarray, act: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - For DDPG: single Q estimate (N,)\n",
    "      - For TD3: (q1, q2, q_min) each (N,)\n",
    "    Works with SB3 >= 1.8 style modules.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    obs_t = torch.as_tensor(obs).float().to(device)\n",
    "    act_t = torch.as_tensor(act).float().to(device)\n",
    "\n",
    "    # DDPG: model.critic\n",
    "    # TD3: model.critic, model.critic_target with two Q nets internally; expose via forward with return both\n",
    "    try:\n",
    "        # TD3 exposes two critics inside model.critic\n",
    "        out = model.critic(obs_t, act_t)\n",
    "        if isinstance(out, tuple) or (hasattr(out, \"__len__\") and len(out) == 2):\n",
    "            q1, q2 = out\n",
    "            q1 = q1.flatten().cpu().numpy()\n",
    "            q2 = q2.flatten().cpu().numpy()\n",
    "            return q1, q2, np.minimum(q1, q2)\n",
    "        else:\n",
    "            q = out.flatten().cpu().numpy()\n",
    "            return q\n",
    "    except Exception:\n",
    "        if hasattr(model.policy, \"q_net\"):\n",
    "            q = model.policy.q_net(obs_t, act_t).flatten().cpu().numpy()\n",
    "            return q\n",
    "        q1 = model.policy.qf1(obs_t, act_t).flatten().cpu().numpy()\n",
    "        q2 = model.policy.qf2(obs_t, act_t).flatten().cpu().numpy()\n",
    "        return q1, q2, np.minimum(q1, q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_and_bias(model, episodes=10, gamma=GAMMA, seed=SEED + 123):\n",
    "    \"\"\"Run deterministic rollouts, collect (obs, action, return-to-go) and compare with critic estimates.\"\"\"\n",
    "    env = gym.make(ENV_ID)\n",
    "    env.reset(seed=seed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    returns = []\n",
    "    all_obs, all_act, all_rtgs = [], [], []\n",
    "    for _ in range(episodes):\n",
    "        obs, info = env.reset(seed=rng.integers(0, 10_000))\n",
    "        done = False\n",
    "        traj_obs, traj_act, traj_rew = [], [], []\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            traj_obs.append(obs)\n",
    "            traj_act.append(action)\n",
    "            traj_rew.append(reward)\n",
    "            obs = next_obs\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # compute per-step return-to-go (MC target) for comparison\n",
    "        G = 0.0\n",
    "        rtgs = []\n",
    "        for r in reversed(traj_rew):\n",
    "            G = r + gamma * G\n",
    "            rtgs.append(G)\n",
    "        rtgs = list(reversed(rtgs))\n",
    "        returns.append(sum(traj_rew))\n",
    "        all_obs.extend(traj_obs)\n",
    "        all_act.extend(traj_act)\n",
    "        all_rtgs.extend(rtgs)\n",
    "    env.close()\n",
    "    all_obs = np.array(all_obs, dtype=np.float32)\n",
    "    all_act = np.array(all_act, dtype=np.float32)\n",
    "    all_rtgs = np.array(all_rtgs, dtype=np.float32)\n",
    "\n",
    "    # critic estimates at (s,a)\n",
    "    q_est = critic_q_estimate(model, all_obs, all_act)\n",
    "    if isinstance(q_est, tuple):\n",
    "        q1, q2, qmin = q_est\n",
    "\n",
    "        # compare to MC return-to-go\n",
    "        bias_q1 = np.mean(q1 - all_rtgs)\n",
    "        bias_q2 = np.mean(q2 - all_rtgs)\n",
    "        bias_qmin = np.mean(qmin - all_rtgs)\n",
    "        return {\n",
    "            \"episode_returns\": returns,\n",
    "            \"mean_return\": float(np.mean(returns)),\n",
    "            \"std_return\": float(np.std(returns)),\n",
    "            \"bias_q1\": float(bias_q1),\n",
    "            \"bias_q2\": float(bias_q2),\n",
    "            \"bias_qmin\": float(bias_qmin),\n",
    "        }\n",
    "    else:\n",
    "        q = q_est\n",
    "        bias = np.mean(q - all_rtgs)\n",
    "        return {\n",
    "            \"episode_returns\": returns,\n",
    "            \"mean_return\": float(np.mean(returns)),\n",
    "            \"std_return\": float(np.std(returns)),\n",
    "            \"bias\": float(bias),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e19c3acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DDPGâ€¦\n",
      "Using cpu device\n",
      "Logging to runs/lunar_1760549383/DDPG_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -472     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 223      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 414      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.89     |\n",
      "|    critic_loss     | 44.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 313      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -348     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 914      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.71     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 813      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -334     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 187      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 1446     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.4     |\n",
      "|    critic_loss     | 16       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | -295     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 183      |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 2090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.1     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 181      |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 2832     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.3     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2731     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | -251     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 3753     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.66     |\n",
      "|    critic_loss     | 7.93     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3652     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 155      |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 4352     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.69     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4251     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 174      |\n",
      "|    ep_rew_mean     | -216     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 5579     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.28     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5478     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 7054     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.4    |\n",
      "|    critic_loss     | 5        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6953     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 202      |\n",
      "|    ep_rew_mean     | -199     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 8063     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 39.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7962     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 8855     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 4.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8754     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-77.22 +/- 23.53\n",
      "Episode length: 145.60 +/- 137.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 146      |\n",
      "|    mean_reward     | -77.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.9    |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 212      |\n",
      "|    ep_rew_mean     | -181     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 10198    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.7    |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10097    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 219      |\n",
      "|    ep_rew_mean     | -173     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 11363    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -36.6    |\n",
      "|    critic_loss     | 22.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11262    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 214      |\n",
      "|    ep_rew_mean     | -168     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 176      |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 11984    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -39.6    |\n",
      "|    critic_loss     | 51.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11883    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 213      |\n",
      "|    ep_rew_mean     | -166     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 12761    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -46.6    |\n",
      "|    critic_loss     | 8.86     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12660    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 220      |\n",
      "|    ep_rew_mean     | -163     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 14088    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -49.3    |\n",
      "|    critic_loss     | 16.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13987    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 217      |\n",
      "|    ep_rew_mean     | -156     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 14755    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -55.2    |\n",
      "|    critic_loss     | 12.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14654    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 234      |\n",
      "|    ep_rew_mean     | -150     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 16836    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.5    |\n",
      "|    critic_loss     | 24       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16735    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 240      |\n",
      "|    ep_rew_mean     | -143     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 18223    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -74.9    |\n",
      "|    critic_loss     | 9.75     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18122    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 249      |\n",
      "|    ep_rew_mean     | -142     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 19940    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -81      |\n",
      "|    critic_loss     | 45.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19839    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-72.36 +/- 34.15\n",
      "Episode length: 132.00 +/- 49.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 132      |\n",
      "|    mean_reward     | -72.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -83.7    |\n",
      "|    critic_loss     | 8.47     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 245      |\n",
      "|    ep_rew_mean     | -141     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 116      |\n",
      "|    total_timesteps | 20612    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -87.1    |\n",
      "|    critic_loss     | 43       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20511    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 256      |\n",
      "|    ep_rew_mean     | -141     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 22557    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -98.7    |\n",
      "|    critic_loss     | 15.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22456    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 261      |\n",
      "|    ep_rew_mean     | -133     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 135      |\n",
      "|    total_timesteps | 24054    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -102     |\n",
      "|    critic_loss     | 25.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23953    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 269      |\n",
      "|    ep_rew_mean     | -134     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 25858    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -112     |\n",
      "|    critic_loss     | 26.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 25757    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 272      |\n",
      "|    ep_rew_mean     | -147     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 153      |\n",
      "|    total_timesteps | 27172    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -124     |\n",
      "|    critic_loss     | 36.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 27071    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 282      |\n",
      "|    ep_rew_mean     | -140     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 161      |\n",
      "|    total_timesteps | 28573    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -131     |\n",
      "|    critic_loss     | 42.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 28472    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 290      |\n",
      "|    ep_rew_mean     | -139     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 29915    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -135     |\n",
      "|    critic_loss     | 110      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-193.94 +/- 121.41\n",
      "Episode length: 293.40 +/- 52.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 293      |\n",
      "|    mean_reward     | -194     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 33.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 309      |\n",
      "|    ep_rew_mean     | -127     |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 175      |\n",
      "|    time_elapsed    | 184      |\n",
      "|    total_timesteps | 32302    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -151     |\n",
      "|    critic_loss     | 39.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 32201    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 315      |\n",
      "|    ep_rew_mean     | -124     |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 174      |\n",
      "|    time_elapsed    | 192      |\n",
      "|    total_timesteps | 33549    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -157     |\n",
      "|    critic_loss     | 37.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33448    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 316      |\n",
      "|    ep_rew_mean     | -120     |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 174      |\n",
      "|    time_elapsed    | 197      |\n",
      "|    total_timesteps | 34396    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -162     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 34295    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 319      |\n",
      "|    ep_rew_mean     | -121     |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 35647    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -162     |\n",
      "|    critic_loss     | 71       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 35546    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 344      |\n",
      "|    ep_rew_mean     | -126     |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 225      |\n",
      "|    total_timesteps | 38770    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -184     |\n",
      "|    critic_loss     | 79.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 38669    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-118.26 +/- 69.85\n",
      "Episode length: 837.00 +/- 326.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 837      |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -194     |\n",
      "|    critic_loss     | 46.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 366      |\n",
      "|    ep_rew_mean     | -124     |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 42179    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 109      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 42078    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 365      |\n",
      "|    ep_rew_mean     | -125     |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 256      |\n",
      "|    total_timesteps | 43508    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -202     |\n",
      "|    critic_loss     | 32       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 43407    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 367      |\n",
      "|    ep_rew_mean     | -124     |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 44734    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -198     |\n",
      "|    critic_loss     | 193      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 44633    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 382      |\n",
      "|    ep_rew_mean     | -125     |\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 278      |\n",
      "|    total_timesteps | 47013    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -204     |\n",
      "|    critic_loss     | 61.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 46912    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=6.48 +/- 134.45\n",
      "Episode length: 355.80 +/- 131.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 356      |\n",
      "|    mean_reward     | 6.48     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -229     |\n",
      "|    critic_loss     | 18.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 403      |\n",
      "|    ep_rew_mean     | -123     |\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 301      |\n",
      "|    total_timesteps | 50466    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 50365    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 410      |\n",
      "|    ep_rew_mean     | -123     |\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 312      |\n",
      "|    total_timesteps | 52338    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -231     |\n",
      "|    critic_loss     | 28.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 52237    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 430      |\n",
      "|    ep_rew_mean     | -123     |\n",
      "| time/              |          |\n",
      "|    episodes        | 156      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 330      |\n",
      "|    total_timesteps | 54974    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -237     |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 54873    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 431      |\n",
      "|    ep_rew_mean     | -124     |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 336      |\n",
      "|    total_timesteps | 55891    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -236     |\n",
      "|    critic_loss     | 148      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 55790    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 432      |\n",
      "|    ep_rew_mean     | -126     |\n",
      "| time/              |          |\n",
      "|    episodes        | 164      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 345      |\n",
      "|    total_timesteps | 57320    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -237     |\n",
      "|    critic_loss     | 52.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 57219    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 443      |\n",
      "|    ep_rew_mean     | -130     |\n",
      "| time/              |          |\n",
      "|    episodes        | 168      |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 356      |\n",
      "|    total_timesteps | 59103    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -236     |\n",
      "|    critic_loss     | 20.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 59002    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-412.16 +/- 321.63\n",
      "Episode length: 615.20 +/- 259.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 615      |\n",
      "|    mean_reward     | -412     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -242     |\n",
      "|    critic_loss     | 119      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 59899    |\n",
      "---------------------------------\n",
      "Training TD3â€¦\n",
      "Using cpu device\n",
      "Logging to runs/lunar_1760549383/TD3_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -484     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 211      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 401      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.32     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 300      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | -498     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 1337     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.83     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1236     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 260      |\n",
      "|    ep_rew_mean     | -410     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 3120     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.85     |\n",
      "|    critic_loss     | 15       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 301      |\n",
      "|    ep_rew_mean     | -327     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 4816     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.63     |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4715     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 293      |\n",
      "|    ep_rew_mean     | -289     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 5858     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.22     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5757     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 304      |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 44       |\n",
      "|    total_timesteps | 7284     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.93     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7183     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | -273     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 8320     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.09     |\n",
      "|    critic_loss     | 9.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8219     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | -261     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 9178     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.02     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9077     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 277      |\n",
      "|    ep_rew_mean     | -248     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 9982     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.6      |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9881     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-177.18 +/- 112.81\n",
      "Episode length: 227.20 +/- 41.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 227      |\n",
      "|    mean_reward     | -177     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.24     |\n",
      "|    critic_loss     | 9.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 269      |\n",
      "|    ep_rew_mean     | -235     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 10752    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.85     |\n",
      "|    critic_loss     | 38.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10651    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 268      |\n",
      "|    ep_rew_mean     | -217     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 11793    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.01     |\n",
      "|    critic_loss     | 26.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11692    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 279      |\n",
      "|    ep_rew_mean     | -211     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 13398    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.01     |\n",
      "|    critic_loss     | 35.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13297    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 301      |\n",
      "|    ep_rew_mean     | -217     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 15639    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.28     |\n",
      "|    critic_loss     | 47.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15538    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 323      |\n",
      "|    ep_rew_mean     | -212     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 111      |\n",
      "|    total_timesteps | 18068    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 8.47     |\n",
      "|    critic_loss     | 25.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 17967    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-161.49 +/- 23.33\n",
      "Episode length: 324.80 +/- 41.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 325      |\n",
      "|    mean_reward     | -161     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 10.5     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 337      |\n",
      "|    ep_rew_mean     | -208     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 125      |\n",
      "|    total_timesteps | 20205    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.4     |\n",
      "|    critic_loss     | 11.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20104    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 359      |\n",
      "|    ep_rew_mean     | -202     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 22989    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 14.6     |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22888    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 390      |\n",
      "|    ep_rew_mean     | -191     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 26490    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 8.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26389    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-125.91 +/- 184.49\n",
      "Episode length: 786.20 +/- 291.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 786      |\n",
      "|    mean_reward     | -126     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.7     |\n",
      "|    critic_loss     | 6.66     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 423      |\n",
      "|    ep_rew_mean     | -179     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 30490    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 13.4     |\n",
      "|    critic_loss     | 8.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 30389    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 444      |\n",
      "|    ep_rew_mean     | -172     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 210      |\n",
      "|    total_timesteps | 33760    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19       |\n",
      "|    critic_loss     | 13.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33659    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 457      |\n",
      "|    ep_rew_mean     | -167     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 36578    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.8     |\n",
      "|    critic_loss     | 8.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 36477    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 475      |\n",
      "|    ep_rew_mean     | -162     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 248      |\n",
      "|    total_timesteps | 39933    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.3     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39832    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-62.32 +/- 87.60\n",
      "Episode length: 924.60 +/- 150.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 925      |\n",
      "|    mean_reward     | -62.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.6     |\n",
      "|    critic_loss     | 8.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 494      |\n",
      "|    ep_rew_mean     | -157     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 43435    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 21.7     |\n",
      "|    critic_loss     | 15.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 43334    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 498      |\n",
      "|    ep_rew_mean     | -153     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 286      |\n",
      "|    total_timesteps | 45852    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.3     |\n",
      "|    critic_loss     | 39.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 45751    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 503      |\n",
      "|    ep_rew_mean     | -149     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 302      |\n",
      "|    total_timesteps | 48297    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.2     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 48196    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-80.86 +/- 82.67\n",
      "Episode length: 544.00 +/- 393.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 544      |\n",
      "|    mean_reward     | -80.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.3     |\n",
      "|    critic_loss     | 7.58     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 504      |\n",
      "|    ep_rew_mean     | -146     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 315      |\n",
      "|    total_timesteps | 50391    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.6     |\n",
      "|    critic_loss     | 14.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 50290    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 529      |\n",
      "|    ep_rew_mean     | -130     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 333      |\n",
      "|    total_timesteps | 53301    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19.2     |\n",
      "|    critic_loss     | 9.87     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 53200    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 544      |\n",
      "|    ep_rew_mean     | -112     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 348      |\n",
      "|    total_timesteps | 55746    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.5     |\n",
      "|    critic_loss     | 31.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 55645    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 547      |\n",
      "|    ep_rew_mean     | -108     |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 361      |\n",
      "|    total_timesteps | 57838    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 17.7     |\n",
      "|    critic_loss     | 15.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 57737    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 540      |\n",
      "|    ep_rew_mean     | -106     |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 367      |\n",
      "|    total_timesteps | 58798    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 15.5     |\n",
      "|    critic_loss     | 13.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 58697    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-66.50 +/- 75.59\n",
      "Episode length: 413.00 +/- 300.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 413      |\n",
      "|    mean_reward     | -66.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 18       |\n",
      "|    critic_loss     | 7.54     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 59899    |\n",
      "---------------------------------\n",
      "\n",
      "Evaluating (mean reward over deterministic rollouts)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bw/Desktop/Dev/SU/IAR/mini-projet-iar/.venv/lib/python3.13/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPG: mean=-307.6 Â± 237.0\n",
      "TD3 : mean=-122.6 Â± 156.1\n",
      "\n",
      "Estimating over-estimation bias on held-out rolloutsâ€¦\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Seed must be a python integer, actual type: <class 'numpy.int64'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mError\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTD3 : mean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtd3_mean\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Â± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtd3_std\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEstimating over-estimation bias on held-out rolloutsâ€¦\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m ddpg_bias = \u001b[43mrollout_and_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mddpg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEVAL_EPISODES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m td3_bias = rollout_and_bias(td3, episodes=EVAL_EPISODES)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Over-estimation summary (critic estimate minus MC return-to-go) ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mrollout_and_bias\u001b[39m\u001b[34m(model, episodes, gamma, seed)\u001b[39m\n\u001b[32m      7\u001b[39m all_obs, all_act, all_rtgs = [], [], []\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     obs, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintegers\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10_000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     11\u001b[39m     traj_obs, traj_act, traj_rew = [], [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dev/SU/IAR/mini-projet-iar/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py:146\u001b[39m, in \u001b[36mTimeLimit.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[32m    137\u001b[39m \n\u001b[32m    138\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m    The reset environment\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28mself\u001b[39m._elapsed_steps = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dev/SU/IAR/mini-projet-iar/.venv/lib/python3.13/site-packages/gymnasium/core.py:333\u001b[39m, in \u001b[36mWrapper.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, seed: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    331\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    332\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dev/SU/IAR/mini-projet-iar/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py:400\u001b[39m, in \u001b[36mOrderEnforcing.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[38;5;28mself\u001b[39m._has_reset = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dev/SU/IAR/mini-projet-iar/.venv/lib/python3.13/site-packages/gymnasium/core.py:333\u001b[39m, in \u001b[36mWrapper.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, seed: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    331\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    332\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dev/SU/IAR/mini-projet-iar/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py:295\u001b[39m, in \u001b[36mPassiveEnvChecker.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_reset_passive_checker(\u001b[38;5;28mself\u001b[39m.env, seed=seed, options=options)\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dev/SU/IAR/mini-projet-iar/.venv/lib/python3.13/site-packages/gymnasium/envs/box2d/lunar_lander.py:329\u001b[39m, in \u001b[36mLunarLander.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    325\u001b[39m     *,\n\u001b[32m    326\u001b[39m     seed: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    327\u001b[39m     options: \u001b[38;5;28mdict\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    328\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mself\u001b[39m._destroy()\n\u001b[32m    332\u001b[39m     \u001b[38;5;66;03m# Bug's workaround for: https://github.com/Farama-Foundation/Gymnasium/issues/728\u001b[39;00m\n\u001b[32m    333\u001b[39m     \u001b[38;5;66;03m# Not sure why the self._destroy() is not enough to clean(reset) the total world environment elements, need more investigation on the root cause,\u001b[39;00m\n\u001b[32m    334\u001b[39m     \u001b[38;5;66;03m# we must create a totally new world for self.reset(), or the bug#728 will happen\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dev/SU/IAR/mini-projet-iar/.venv/lib/python3.13/site-packages/gymnasium/core.py:157\u001b[39m, in \u001b[36mEnv.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Initialize the RNG if the seed is manually passed\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     \u001b[38;5;28mself\u001b[39m._np_random, \u001b[38;5;28mself\u001b[39m._np_random_seed = \u001b[43mseeding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnp_random\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Dev/SU/IAR/mini-projet-iar/.venv/lib/python3.13/site-packages/gymnasium/utils/seeding.py:31\u001b[39m, in \u001b[36mnp_random\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(seed, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m0\u001b[39m <= seed):\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(seed, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m error.Error(\n\u001b[32m     32\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSeed must be a python integer, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(seed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m         )\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     35\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m error.Error(\n\u001b[32m     36\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSeed must be greater or equal to zero, actual value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m         )\n",
      "\u001b[31mError\u001b[39m: Seed must be a python integer, actual type: <class 'numpy.int64'>"
     ]
    }
   ],
   "source": [
    "logdir = f\"runs/lunar_{int(time.time())}\"\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "print(\"Training DDPGâ€¦\")\n",
    "ddpg, _ = train_model(\"DDPG\", logdir, TIMESTEPS)\n",
    "print(\"Training TD3â€¦\")\n",
    "td3, _ = train_model(\"TD3\", logdir, TIMESTEPS)\n",
    "\n",
    "print(\"\\nEvaluating (mean reward over deterministic rollouts)â€¦\")\n",
    "ddpg_mean, ddpg_std = evaluate_policy(\n",
    "    ddpg,\n",
    "    gym.make(ENV_ID),\n",
    "    n_eval_episodes=EVAL_EPISODES,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n",
    "td3_mean, td3_std = evaluate_policy(\n",
    "    td3,\n",
    "    gym.make(ENV_ID),\n",
    "    n_eval_episodes=EVAL_EPISODES,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n",
    "\n",
    "print(f\"DDPG: mean={ddpg_mean:.1f} Â± {ddpg_std:.1f}\")\n",
    "print(f\"TD3 : mean={td3_mean:.1f} Â± {td3_std:.1f}\")\n",
    "\n",
    "print(\"\\nEstimating over-estimation bias on held-out rolloutsâ€¦\")\n",
    "ddpg_bias = rollout_and_bias(ddpg, episodes=EVAL_EPISODES)\n",
    "td3_bias = rollout_and_bias(td3, episodes=EVAL_EPISODES)\n",
    "\n",
    "print(\"\\n--- Over-estimation summary (critic estimate minus MC return-to-go) ---\")\n",
    "if \"bias\" in ddpg_bias:\n",
    "    print(\n",
    "        f\"DDPG: bias={ddpg_bias['bias']:+.2f}, eval return={ddpg_bias['mean_return']:.1f} Â± {ddpg_bias['std_return']:.1f}\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"DDPG (q1/q2/min): {ddpg_bias}\")  # should be single 'bias' for DDPG\n",
    "\n",
    "# TD3 has q1, q2, and min\n",
    "print(\n",
    "    f\"TD3 : bias_q1={td3_bias['bias_q1']:+.2f}, bias_q2={td3_bias['bias_q2']:+.2f}, bias_qmin={td3_bias['bias_qmin']:+.2f}, \"\n",
    "    f\"eval return={td3_bias['mean_return']:.1f} Â± {td3_bias['std_return']:.1f}\"\n",
    ")\n",
    "\n",
    "print(\"\\nDone. You can inspect TensorBoard logs with:\")\n",
    "print(f\"tensorboard --logdir {logdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522c2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
